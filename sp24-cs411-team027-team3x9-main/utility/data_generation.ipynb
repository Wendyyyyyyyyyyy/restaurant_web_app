{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain import PromptTemplate\n",
    "from openai import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Subsets of Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subset(input_filename, output_filename, subset_size=10000):\n",
    "    subset = []\n",
    "    with open(input_filename, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= subset_size:  # Stop after reading subset_size objects\n",
    "                break\n",
    "            try:\n",
    "                obj = json.loads(line)  # Try to parse each line as a JSON object\n",
    "                subset.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from line {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    with open(output_filename, 'w') as outfile:\n",
    "        json.dump(subset, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_filename = '/Users/justin/Downloads/yelp_dataset/yelp_academic_dataset_business.json'\n",
    "output_filename = '/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/restaurant_10000.json'\n",
    "generate_subset(input_filename, output_filename)\n",
    "\n",
    "input_filename = '/Users/justin/Downloads/yelp_dataset/yelp_academic_dataset_user.json'\n",
    "output_filename = '/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/user_10000.json'\n",
    "generate_subset(input_filename, output_filename)\n",
    "\n",
    "input_filename = '/Users/justin/Downloads/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "output_filename = '/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/review_10000.json'\n",
    "generate_subset(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tastes = [\n",
    "    \"Spicy\",\n",
    "    \"Sweet\",\n",
    "    \"Sour\",\n",
    "    \"Salty\",\n",
    "    \"Umami\",\n",
    "    \"Bitter\",\n",
    "    \"Earthy\",\n",
    "    \"Smokey\",\n",
    "    \"Herbal\",\n",
    "    \"Fruity\",\n",
    "    \"Nutty\",\n",
    "    \"Buttery\",\n",
    "    \"Cheesy\",\n",
    "    \"Tangy\",\n",
    "    \"Savory\",\n",
    "    \"Rich\",\n",
    "    \"Creamy\",\n",
    "    \"Zesty\",\n",
    "    \"Mild\",\n",
    "    \"Fiery\",\n",
    "    \"Crispy\",\n",
    "    \"Tender\",\n",
    "    \"Juicy\",\n",
    "    \"Dry\",\n",
    "    \"Moist\",\n",
    "    \"Fluffy\",\n",
    "    \"Crunchy\",\n",
    "    \"Gooey\",\n",
    "    \"Chewy\",\n",
    "    \"Frothy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operating_hours_to_string(hours_dict):\n",
    "    days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    summary_str = \"\"\n",
    "    for day in days_order:\n",
    "        if day in hours_dict:\n",
    "            summary_str += f\"{day}: {hours_dict[day]}\\n\"\n",
    "        else:\n",
    "            summary_str += f\"{day}: Closed\\n\"  # Assuming closed if not specified\n",
    "    return summary_str.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_city_indices(city_list):\n",
    "    cities = set(dic[\"city\"] for dic in city_list if \"city\" in dic)\n",
    "    sorted_cities = sorted(cities)\n",
    "    city_indices = {city: index for index, city in enumerate(sorted_cities)}\n",
    "    return city_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_restaurant(input_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Restaurants(RestaurantId:INT[PK], \n",
    "    RestaurantName:VARCHAR(255), \n",
    "    Stars:REAL, \n",
    "    Hours:VARCHAR(255), \n",
    "    Category:VARCHAR(127), \n",
    "    LocationId:VARCHAR(255)[FK to Location.LocationId])\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    original_file = read_json(input_filename)\n",
    "    \n",
    "    city_indices = assign_city_indices(original_file)\n",
    "    \n",
    "    for i in range(len(original_file)):\n",
    "        new_sample = {}\n",
    "        new_sample[\"RestaurantId\"] = i+1\n",
    "        # new_sample[\"LocationId\"] = city_indices[original_file[i][\"city\"]]\n",
    "        new_sample[\"LocationId\"] = random.randint(1,1000)\n",
    "        new_sample[\"Hours\"] = operating_hours_to_string(original_file[i][\"hours\"])\n",
    "        new_sample[\"Stars\"] = original_file[i][\"stars\"]\n",
    "        # new_sample[\"Category\"] = original_file[i][\"categories\"]\n",
    "        new_sample[\"Category\"] = tastes[random.randint(0, 29)]\n",
    "        new_sample[\"RestaurantName\"] = original_file[i][\"name\"]\n",
    "        output.append(new_sample)\n",
    "\n",
    "        \n",
    "    write_json(output,output_filename)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_restaurant(\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/restaurant_10000.json\", \"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/restaurant_10000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_city_indices(city_list):\n",
    "    cities = set(dic[\"city\"] for dic in city_list if \"city\" in dic)\n",
    "    sorted_cities = sorted(cities)\n",
    "    city_indices = {city: index for index, city in enumerate(sorted_cities)}\n",
    "    return city_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_json(\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/user.json\")\n",
    "new_data = []\n",
    "i = 1\n",
    "for sample in data:\n",
    "    new_sample = {}\n",
    "    new_sample[\"user_id\"] = i\n",
    "    new_sample[\"name\"] = sample[\"name\"]\n",
    "    new_sample[\"Taste\"] = tastes[random.randint(0, 29)]\n",
    "    new_sample[\"Password\"] = sample[\"user_id\"]\n",
    "    i+=1\n",
    "    new_data.append(new_sample)\n",
    "write_json(new_data, \"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/user.json\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RestaurantId:INT[FK to Restaurant.RestaurantId], \n",
    "UserId:INT[FK to User.UserId], \n",
    "Date:DATETIME[PK], \n",
    "Star:REAL, \n",
    "Text:MEDIUMTEXT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_review(restaurant_data_path, user_data_path, input_filename, output_filename):\n",
    "    \"\"\"\n",
    "    RestaurantId:INT[FK to Restaurant.RestaurantId], \n",
    "    UserId:INT[FK to User.UserId], \n",
    "    Date:DATETIME[PK], \n",
    "    Star:REAL, \n",
    "    Text:MEDIUMTEXT\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    original_file = read_json(input_filename)\n",
    "    restaurant_data = read_json(restaurant_data_path)\n",
    "    user_data = read_json(user_data_path)\n",
    "    \n",
    "    \n",
    "    for i in range(len(original_file)):\n",
    "        new_sample = {}\n",
    "        new_sample[\"RestaurantId\"] = restaurant_data[i][\"RestaurantId\"]\n",
    "        new_sample[\"UserId\"] = user_data[i][\"UserId\"]\n",
    "        new_sample[\"Stars\"] = original_file[i][\"stars\"]\n",
    "        new_sample[\"Text\"] = original_file[i][\"text\"]\n",
    "        new_sample[\"Date\"] = original_file[i][\"date\"]\n",
    "        output.append(new_sample)\n",
    "\n",
    "        \n",
    "    write_json(output,output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_review(user_data_path=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/user.json\", \n",
    "                restaurant_data_path=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/restaurant.json\",\n",
    "                input_filename=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/review.json\",\n",
    "                output_filename=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/review.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_location(restaurant_data_path, original_restaurant_data_path, output_filename):\n",
    "    '''\n",
    "    LocationId:INT[PK], \n",
    "    Latitude:REAL, \n",
    "    Longitude:REAL, \n",
    "    PostalCode:VARCHAR(31), \n",
    "    State:VARCHAR(15), \n",
    "    City:VARCHAR(31)\n",
    "    '''\n",
    "    \n",
    "    output = []\n",
    "    restaurant_data = read_json(restaurant_data_path)\n",
    "    original_restaurant_data = read_json(original_restaurant_data_path)\n",
    "    \n",
    "    \n",
    "    for i in range(len(restaurant_data)):\n",
    "        new_sample = {}\n",
    "        new_sample[\"LocationId\"] = restaurant_data[i][\"LocationId\"]\n",
    "        new_sample[\"Latitude\"] = original_restaurant_data[i][\"latitude\"]\n",
    "        new_sample[\"Longitude\"] = original_restaurant_data[i][\"longitude\"]\n",
    "        new_sample[\"PostalCode\"] = original_restaurant_data[i][\"postal_code\"]\n",
    "        new_sample[\"State\"] = original_restaurant_data[i][\"state\"]\n",
    "        new_sample[\"City\"] = original_restaurant_data[i][\"city\"]\n",
    "        output.append(new_sample)\n",
    "\n",
    "        \n",
    "    write_json(output,output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_location(restaurant_data_path=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/restaurant.json\", original_restaurant_data_path=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/original_data/restaurant.json\", output_filename=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/location.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = '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')\n",
    "\n",
    "def get_dish_name_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a naming helper.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate names for 5 dishes or items suitable for a restaurant or store called {prompt}.1, seperate the names by ';', 2, just return the names and nothing else.\"},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Ocean's Delight Platter; \\n2. Seaside Bounty Pasta; \\n3. Captain's Catch Tacos; \\n4. Neptune's Feast Bowl; \\n5. Marine Delight Sushi Rolls\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dish_name_completion(\"sea food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dish_names(s):\n",
    "    lines = s.split(\"\\n\")\n",
    "    dish_names = [line.split(\". \", 1)[1].split(\";\")[0] for line in lines if \". \" in line]\n",
    "    return dish_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ocean Bounty Platter',\n",
       " 'Seashell Surprise Soup',\n",
       " 'Maritime Medley Salad',\n",
       " \"Neptune's Delight Pasta\",\n",
       " 'Coastal Catch Tacos',\n",
       " 'Lighthouse Lobster Roll',\n",
       " 'Seaside Scallops Stir-fry',\n",
       " 'Coral Reef Ceviche',\n",
       " 'Tidal Wave Tempura',\n",
       " \"Captain's Choice Clambake\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_dish_names(\"1. Ocean Bounty Platter; \\n2. Seashell Surprise Soup; \\n3. Maritime Medley Salad; \\n4. Neptune's Delight Pasta; \\n5. Coastal Catch Tacos; \\n6. Lighthouse Lobster Roll; \\n7. Seaside Scallops Stir-fry; \\n8. Coral Reef Ceviche; \\n9. Tidal Wave Tempura; \\n10. Captain's Choice Clambake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate(restaurant_data_path, output_filename):\n",
    "    \"\"\"\n",
    "    DishId:INT[PK], \n",
    "    RestaurantId:INT[FK to Restaurant.RestaurantId], \n",
    "    Price:REAL, \n",
    "    Name:VARCHAR(127)\n",
    "    \"\"\"\n",
    "    \n",
    "    output = []\n",
    "    restaurant_data = read_json(restaurant_data_path)\n",
    "    \n",
    "    j=0\n",
    "    for i in tqdm(range(len(restaurant_data))):\n",
    "        name_list = extract_dish_names(get_dish_name_completion(restaurant_data[i][\"RestaurantName\"]))\n",
    "        for dishname in name_list:\n",
    "            \n",
    "            new_sample = {}\n",
    "            new_sample[\"DishId\"] = j+1\n",
    "            new_sample[\"RestaurantId\"] = restaurant_data[i][\"RestaurantId\"]\n",
    "            new_sample[\"Price\"] = random.randint(5,15)\n",
    "            new_sample[\"Name\"] = dishname\n",
    "            output.append(new_sample)\n",
    "\n",
    "        \n",
    "    write_json(output,output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [2:39:12<00:00,  1.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "generate(\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/restaurant_10000.json\", \"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/dish_50000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21855\n"
     ]
    }
   ],
   "source": [
    "sample = read_json(\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/dish_50000.json\")\n",
    "print(len(sample))\n",
    "for i in range(len(sample)):\n",
    "    sample[i][\"DishId\"] = i+1\n",
    "write_json(sample, \"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/dish_50000.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_crime_indices(location_list):\n",
    "    postals = set(dic[\"PostalCode\"] for dic in location_list if \"PostalCode\" in dic)\n",
    "    sorted_postals = sorted(postals)\n",
    "    dic = {}\n",
    "    for index, postal in enumerate(sorted_postals):\n",
    "        dic[postal] = random.randint(0, 50)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_crime(location_data_path, output_filename):\n",
    "    \"\"\"\n",
    "    Crimes(CrimeId:INT[PK]), \n",
    "    Count:INT, \n",
    "    Type:INT, \n",
    "    Cleared:BOOLEAN, \n",
    "    LocationId:INT[FK to Location.LocationId])\n",
    "    \"\"\"\n",
    "    \n",
    "    output = []\n",
    "    location_data = read_json(location_data_path)\n",
    "    \n",
    "    \n",
    "    for i in range(10000):\n",
    "        new_sample = {}\n",
    "        new_sample[\"CrimeId\"] = i+1\n",
    "        new_sample[\"Count\"] = random.randint(0,30)\n",
    "        new_sample[\"Type\"] = random.randint(0,6)\n",
    "        random_boolean = random.choice([0, 1])\n",
    "        new_sample[\"Cleared\"] = random_boolean\n",
    "        new_sample[\"LocationId\"] = random.randint(1, 1000)\n",
    "        \n",
    "        output.append(new_sample)\n",
    "\n",
    "        \n",
    "    write_json(output,output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_crime(\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/location.json\", \"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/crime_10000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occurence(input, output_filename):\n",
    "    \"\"\"\n",
    "    RestaurantId:INT[PK], \n",
    "    CrimeId:INT[PK]\n",
    "    \"\"\"\n",
    "    output=[]\n",
    "    crime = read_json(input)\n",
    "    for i in tqdm(range(len(crime))):\n",
    "        new_sample = {}\n",
    "        new_sample[\"LocationId\"] = crime[i][\"LocationId\"]\n",
    "        new_sample[\"CrimeId\"] =crime[i][\"CrimeId\"]\n",
    "        \n",
    "        output.append(new_sample)\n",
    "        \n",
    "    write_json(output,output_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 3315393.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_occurence(input = \"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/crime_10000.json\",output_filename=\"/Users/justin/Desktop/sp24-cs411-team027-team3x9/data/occurence_10000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_json_to_csv(source_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Converts all JSON files in the source directory to CSV format and saves them in the target directory.\n",
    "\n",
    "    Args:\n",
    "    - source_dir (str): The directory containing the JSON files.\n",
    "    - target_dir (str): The directory where the CSV files will be saved.\n",
    "    \"\"\"\n",
    "    # Create the target directory if it does not exist\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Loop through all files in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            # Construct the full file path\n",
    "            json_path = os.path.join(source_dir, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted crime.json to CSV.\n",
      "Converted dish.json to CSV.\n",
      "Converted restaurant.json to CSV.\n",
      "Converted location.json to CSV.\n",
      "Converted restaurant_10000.json to CSV.\n",
      "Converted user.json to CSV.\n",
      "Converted occurence_10000.json to CSV.\n",
      "Converted crime_10000.json to CSV.\n",
      "Converted review.json to CSV.\n",
      "Converted occurence.json to CSV.\n",
      "Converted dish_50000.json to CSV.\n"
     ]
    }
   ],
   "source": [
    "def convert_json_to_csv(source_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Converts all JSON files in the source directory to CSV format and saves them in the target directory.\n",
    "\n",
    "    Args:\n",
    "    - source_dir (str): The directory containing the JSON files.\n",
    "    - target_dir (str): The directory where the CSV files will be saved.\n",
    "    \"\"\"\n",
    "    # Create the target directory if it does not exist\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Loop through all files in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            # Construct the full file path\n",
    "            json_path = os.path.join(source_dir, filename)\n",
    "            csv_path = os.path.join(target_dir, filename.replace('.json', '.csv'))\n",
    "            \n",
    "            # Load the JSON data\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "            \n",
    "            # Convert the JSON data to a DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Save the DataFrame to a CSV file\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            print(f\"Converted {filename} to CSV.\")\n",
    "\n",
    "# Example usage:\n",
    "source_directory = '/Users/justin/Desktop/sp24-cs411-team027-team3x9/data'\n",
    "target_directory = '/Users/justin/Desktop/sp24-cs411-team027-team3x9/data_csv'\n",
    "convert_json_to_csv(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
